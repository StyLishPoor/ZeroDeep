<?xml version="1.0" encoding="UTF-8"?>
<html xmlns:epub='http://www.idpf.org/2007/ops' xml:lang='ja' xmlns:ops='http://www.idpf.org/2007/ops' xmlns='http://www.w3.org/1999/xhtml'>
<head>
  <meta charset='UTF-8'/>
  <link href='oreilly.css' rel='stylesheet' type='text/css'/>
  <meta content='Re:VIEW' name='generator'/>
  <title>ディープラーニング</title>
</head>
<body>
<h1 id='h8'><span class='chapno'>8章</span><br/>ディープラーニング</h1>
<p>ディープラーニングは、層を深くしたディープなニューラルネットワークです。これまで説明してきたネットワークをベースに、後は層を重ねるだけで、ディープなネットワークを作ることができます。しかし、ディープなネットワークには課題もあります。本章では、ディープラーニングの性質と課題、そして可能性について見ていきます。また、現在のディープラーニングについて俯瞰した説明を行います。</p>

<h2 id='h8-1'><span class='secno'>8.1　</span>ネットワークをより深く</h2>
<p>ニューラルネットワークに関して、これまで多くのことを学びました。たとえば、ニューラルネットワークを構成するさまざまな層や、学習を行う上で有効なテクニック、画像系に特に有効なCNNや、パラメータの最適化手法などです。それらはどれもディープラーニングにおいて重要な技術です。ここでは、これまで学んだ技術を集約して、ディープなネットワークを作り、MNISTデータセットの手書き数字認識に挑みたいと思います。</p>

<h3 id='h8-1-1'><span class='secno'>8.1.1　</span>よりディープなネットワークへ</h3>
<p>早速ですが、ここでは<a href='./ch08.xhtml#fig08_1'>図8-1</a>のネットワーク構成からなるCNN——これまでよりもディープなネットワーク——を作りたいと思います。なお、このネットワークは、次節で説明するVGGというネットワークを参考にしています。</p>
<div class='image' id='fig08_1'>
<img alt='手書き数字認識を行うディープなCNN' src='images/ch08/fig08_1.png'/>
<p class='caption'>
図8-1　手書き数字認識を行うディープなCNN
</p>
</div>
<p><a href='./ch08.xhtml#fig08_1'>図8-1</a>に示すとおり、これまで実装してきたネットワークよりも層がディープになっています。ここで使用する畳み込み層はすべて3×3の小さなフィルターで、層が深くなるにつれてチャンネル数が大きくなるのが特徴です（畳み込み層のチャンネル数は、前層から順に、16、16、32、32、64、64と増えていきます）。また、図に示すとおり、プーリング層を挿入し中間データの空間サイズを徐々に小さくしていきます。そして、後段の全結合層では、Dropoutレイヤを使用します。</p>
<p>このネットワークでは、重みの初期値として「Heの初期値」を使用し、重みパラメータの更新にAdamを用います。以上をまとめると、このネットワークの特徴としては次の点が挙げられます。</p>
<ul>
<li>3×3の小さなフィルターによる畳み込み層</li>
<li>活性化関数はReLU</li>
<li>全結合層の後にDropoutレイヤを使用</li>
<li>Adamによる最適化</li>
<li>重みの初期値として「Heの初期値」を使用</li>
</ul>
<p>これらの特徴が物語るように、<a href='./ch08.xhtml#fig08_1'>図8-1</a>のネットワークには、これまで学んだニューラルネットワークの技術が多く使われています。それでは、このネットワークを使って、学習を行ってみましょう。結果を先に言うと、このネットワークの認識精度は99.38% <a class='noteref' href='#fn-fn0801' id='fnb-fn0801' epub:type='noteref'>†1</a>になります。これは、とても素晴らしい性能と言えるでしょう！</p>
<div class='footnote' id='fn-fn0801' epub:type='footnote'><p class='footnote'>[†1] 最終的な認識精度には多少のばらつきがあります。ただし、今回のネットワークでは、概ね99%を超える結果になるでしょう。</p></div>
<div class='note'><table class='note'><tr><td class='center top' rowspan='2'><img alt='[注記]' class='noteicon' src='images/note.png'/></td></tr><tr><td>
<p><a href='./ch08.xhtml#fig08_1'>図8-1</a>のネットワークを実装したソースコードは<code class='tt'>ch08/deep_convnet.py</code>にあります。また、訓練用のコードは、<code class='tt'>ch08/train_deepnet.py</code>に用意しています。それらのコードを用いれば、ここで行った学習は再現できますが、ディープなネットワークの学習には多くの時間（おそらく半日以上）が必要になります。本書では、学習済みの重みパラメータを<code class='tt'>ch08/deep_conv_net_params.pkl</code>として与えています。先の<code class='tt'>deep_convnet.py</code>は、学習済みのパラメータを読み込む機能を備えているので、適宜利用してください。</p>
</td></tr></table></div>
<p><a href='./ch08.xhtml#fig08_1'>図8-1</a>のネットワークの誤認識率はわずか0.62%です。ここでは、どのような画像に対して認識を誤ったのか、実際に見てみることにしましょう。<a href='./ch08.xhtml#fig08_2'>図8-2</a>に、実際に認識を誤った例を示します。</p>
<div class='image' id='fig08_2'>
<img alt='認識を誤った画像の例：各画像の左上に正解ラベル、右下に本ネットワークの推論の結果を示す' src='images/ch08/fig08_2.png'/>
<p class='caption'>
図8-2　認識を誤った画像の例：各画像の左上に正解ラベル、右下に本ネットワークの推論の結果を示す
</p>
</div>
<p><a href='./ch08.xhtml#fig08_2'>図8-2</a>を見て分かるとおり、これらの画像は私たち人間にとっても判断が難しい画像です。実際、何の数字か判断が難しいケースや、私たちも同じように“認識ミス”を犯す画像がいくつも含まれていることが分かります。たとえば、左上の画像（正解は「6」）は「0」に見えますし、その隣の画像（正解は「3」）は確かに「5」にも見えます。全体的に「1」と「7」、「0」と「6」、「3」と「5」の組み合わせが紛らわしいようですが、このような例を見ていくと、認識を誤ったのも納得できると思います。</p>
<p>今回のディープなCNNは、高精度でありながら、認識を誤った画像に対しても、人間と同じような“認識ミス”を犯しています。このような点からも、ディープなCNNには大きな可能性を感じることができるでしょう。</p>

<h3 id='h8-1-2'><span class='secno'>8.1.2　</span>さらに認識精度を高めるには</h3>
<p>「What is the class of this image ?」というタイトルのWebサイト<u>［32］</u>には、さまざまなデータセットを対象に、これまで論文などで発表されてきた手法の認識精度がランキング形式で掲載されています（<a href='./ch08.xhtml#fig08_3'>図8-3</a>）。</p>
<div class='image' id='fig08_3'>
<img alt='MNISTデータセットに対する各手法のランキング（文献&lt;u&gt;［32］&lt;/u&gt;より引用：2016年6月時点）' src='images/ch08/fig08_3.png'/>
<p class='caption'>
図8-3　MNISTデータセットに対する各手法のランキング（文献<u>［32］</u>より引用：2016年6月時点）
</p>
</div>
<p><a href='./ch08.xhtml#fig08_3'>図8-3</a>のランキング結果を見てみると、「Neural Networks」や「Deep」「Convolutional」というキーワードが目立ちます。実際のところ、ランキングの上位を占めている手法の多くは、CNNをベースとした手法なのです。ちなみに、2016年6月時点のMNISTデータセットに対する最高の認識精度は99.79%（誤認識率0.21%）で、その手法もCNNをベースとしています<u>［33］</u>。ただし、そこで使用するCNNは、さほどディープなネットワークではありません（畳み込み層が2層、全結合層が2層程度のネットワーク）。</p>
<div class='note'><table class='note'><tr><td class='center top' rowspan='2'><img alt='[注記]' class='noteicon' src='images/note.png'/></td></tr><tr><td>
<p>MNISTデータセットに対しては、層をそこまで深くせずに（現時点では）最高精度の結果が得られています。これは、手書き数字という比較的単純な問題に対しては、ネットワークの表現力をそこまで高める必要がないからだと考えられます。そのため、層を深くすることの恩恵が少ないと言えるでしょう。この後に紹介する大規模な一般物体認識では、問題が複雑になるため、層を深くすることが認識精度の向上に大いに貢献することが分かります。</p>
</td></tr></table></div>
<p>先のランキングの上位の手法を参考にすると、認識精度をさらに高めるための技術やヒントを発見できるでしょう。たとえば、アンサンブル学習、学習係数の減衰（learning rate decay）、<!-- IDX:Data Augmentation --><b>Data Augmentation</b>（データ拡張）などは、認識精度の向上に貢献していることが分かります。特に、Data Augmentationは簡単な手法でありながら、認識精度を向上させる上で特に有効な方法です。</p>
<p>Data Augmentationは、入力画像（訓練画像）をアルゴリズムによって“人工的”に拡張します。具体的に言うと、<a href='./ch08.xhtml#fig08_4'>図8-4</a>に示すように、入力画像に対して、回転や縦横方向の移動などの微小な変化を与え、画像枚数を増やすことを行います。これは、データセットの枚数が限られている場合には特に有効な手段です。</p>
<div class='image' id='fig08_4'>
<img alt='Data Augmentationの例' src='images/ch08/fig08_4.png'/>
<p class='caption'>
図8-4　Data Augmentationの例
</p>
</div>
<p>Data Augmentationは、<a href='./ch08.xhtml#fig08_4'>図8-4</a>のような変形以外にも、さまざまな方法で画像を拡張することができます。たとえば、画像の中から一部を切り出す「<!-- IDX:crop処理 -->crop処理」や、左右をひっくり返す「<!-- IDX:flip処理 -->flip処理<a class='noteref' href='#fn-fn0802' id='fnb-fn0802' epub:type='noteref'>†2</a>」などです。また、一般的な画像では、輝度などの見た目の変化や拡大・縮小などのスケール変化をつけることも有効です。いずれにせよ、Data Augmentationによって訓練画像をうまく増やすことができれば、ディープラーニングの認識精度を向上させることができます。これは簡単な“トリック”に思えるかもしれませんが、良い結果をもたらすことが多くあります。ここではData Augmentationの実装は行いませんが、この“トリック”の実装は簡単に行えますので、興味のある方は試してみてください。</p>
<div class='footnote' id='fn-fn0802' epub:type='footnote'><p class='footnote'>[†2] flip処理は、画像の対称性を考慮する必要のない場合にのみ有効です。</p></div>

<h3 id='h8-1-3'><span class='secno'>8.1.3　</span>層を深くすることのモチベーション</h3>
<p>「層を深くすること」の重要性については、理論的にはそれほど多くのことが分かっていないのが現状です。理論的側面は現在のところ乏しいにしろ、これまでの研究や実験から、説明できることはいくつかあります（やや直感的にではあるにせよ）。ここでは、「層を深くすること」の重要性について、それを補強するデータや説明をいくつか与えたいと思います。</p>
<p>まず初めに、層を深くすることの重要性は、ILSVRCに代表される大規模画像認識のコンペティションの結果から汲み取ることができます（詳細は次節を参照）。そのようなコンペティションの結果が示すところでは、最近の上位を占める手法の多くはディープラーニングによる手法であり、傾向としては、ネットワークの層を深くする方向へ向かっています。つまり、層を深くすることに比例して、認識性能も向上していると読み取れます。</p>
<p>続いて、層を深くすることの利点について述べます。層を深くすることの利点のひとつは、ネットワークのパラメータ数を少なくできることです。より詳しく言えば、層を深くしたネットワークは、層を深くしなかった場合に比べて、より少ないパラメータで同レベル（もしくはそれ以上）の表現力を達成できるのです。このことは、畳み込み演算でのフィルターサイズに着目して考えてみると分かりやすいでしょう。たとえば、5×5のフィルターからなる畳み込み層の例を<a href='./ch08.xhtml#fig08_5'>図8-5</a>に示します。</p>
<div class='image' id='fig08_5'>
<img alt='5×5の畳み込み演算の例' src='images/ch08/fig08_5.png'/>
<p class='caption'>
図8-5　5×5の畳み込み演算の例
</p>
</div>
<p>ここで注目してほしい点は、出力データの各ノードは、入力データのどの領域から計算されているか、ということです。当たり前ですが、<a href='./ch08.xhtml#fig08_5'>図8-5</a>の例では、出力ノードひとつあたり、入力データの5×5の領域から計算されることになります。続いて、<a href='./ch08.xhtml#fig08_6'>図8-6</a>のように、3×3の畳み込み演算を2回繰り返して行う場合を考えます。この場合、出力ノードひとつあたり、中間データでは3×3の領域から計算されます。それでは、中間データの3×3の領域は、ひとつ前の入力データのどの領域から計算されるでしょうか？ <a href='./ch08.xhtml#fig08_6'>図8-6</a>をよく見て考えれば、それは5×5の領域に対応することが分かります。つまり、<a href='./ch08.xhtml#fig08_6'>図8-6</a>の出力データは、入力データの5×5の領域を“見て”計算することになるのです。</p>
<div class='image' id='fig08_6'>
<img alt='3×3の畳み込み層を2回繰り返した場合の例' src='images/ch08/fig08_6.png'/>
<p class='caption'>
図8-6　3×3の畳み込み層を2回繰り返した場合の例
</p>
</div>
<p>5×5の畳み込み演算1回の領域は、3×3の畳み込み演算を2回行うことでカバーできます。しかも、前者のパラメータ数が25（5×5）であるのに対して、後者は合計18（2×3×3）であり、パラメータ数は畳み込み層を重ねて行ったほうが少なくなります。そして、そのパラメータ数の差は、層が深くなるにつれて大きくなります。たとえば、3×3の畳み込み演算を3回繰り返す場合のパラメータ数は全部で27個になりますが、それと同じ領域を1回の畳み込み演算で“見る”ためには7×7のフィルターが必要であり、そのときのパラメータ数は49個になります。</p>
<div class='note'><table class='note'><tr><td class='center top' rowspan='2'><img alt='[注記]' class='noteicon' src='images/note.png'/></td></tr><tr><td>
<p>小さなフィルターを重ねてネットワークを深くすることの利点は、パラメータ数を小さくでき、<!-- IDX:受容野 --><b>受容野</b>（receptive field）を広くカバーできる点にあります（受容野とは、ニューロンに変化を生じさせる局所的な空間領域のこと）。さらに、層を重ねることで、ReLUなどの活性化関数が畳み込み層の間に挟まれることになり、ネットワークの表現力がさらに向上します。これは、活性化関数によってネットワークに「非線形」の力が加わるからであり、非線形の関数が重なることで、さらに複雑な表現が可能になるのです。</p>
</td></tr></table></div>
<p>学習の効率性も、層を深くすることの利点のひとつです。これは、層を深くしなかった場合に比べて、層を深くすることで、学習データを少なくでき、高速に学習が行えることを意味します。これを（直感的に）理解するには、<a href='ch07.xhtml#h7-6'>「7.6 CNNの可視化」</a>で説明したことを思い出すとよいでしょう。7.6節では、CNNの畳み込み層が階層的に情報を抽出していることを説明しました。具体的には、前層の畳み込み層では、エッジなどの単純な形状にニューロンが反応し、層が深くなるにつれて、テクスチャや物体のパーツといったように、階層的に複雑になっていくことを説明しました。</p>
<p>そのようなネットワークの階層構造を頭に置きながら、「犬」を認識する問題について考えてみましょう。この問題を浅いネットワークで解決しようとすれば、畳み込み層は「犬」の特徴の多くを一度に“理解”する必要があるでしょう。「犬」にはさまざまな種類があり、撮影される環境によって見え方も大きく変化します。そのため、「犬」の特徴を理解するためには、多くのバリエーションに富んだ学習データが必要になり、それによって、学習に多くの時間が必要になります。</p>
<p>しかし、ネットワークを深くすれば、学習すべき問題を階層的に分解することができます。そのため、各層が学習すべき問題は、より単純な問題として取り組むことができるのです。これは、たとえば、最初の層はエッジだけを学習することに専念すればよいということになり、少ない学習データで効率良く学習を行うことができるのです。なぜなら、「犬」が写っている画像に比べて、エッジを含む画像はたくさん存在するから、そして、エッジのパターンは「犬」のパターンよりも簡単な構造であるからです。</p>
<p>また、層を深くすることで階層的に情報を渡していくことができる点も重要です。たとえば、エッジを抽出した層の次の層は、エッジ情報を使えるので、より高度なパターンを効率良く学習できることが期待できます。つまり、層を深くすることで、各層が学習すべき問題を「解きやすいシンプルな問題」へと分解することができ、効率良く学習することが期待できるのです。</p>
<p>以上が、層を深くすることの重要性を補強する説明になります。ただし、ここでの注意点としては、近年の層のディープ化は、層を深くしても正しく学習できるだけの新たな技術や環境——ビッグデータやコンピュータパワーなど——によってもたらされたことを強調しておきます。</p>

<h2 id='h8-2'><span class='secno'>8.2　</span>ディープラーニングの小歴史</h2>
<p>ディープラーニングが現在のように大きな注目を集めるきっかけになったのは、2012年に開催された大規模画像認識のコンペティション<!-- IDX:ILSVRC -->ILSVRC（ImageNet Large Scale Visual Recognition Challenge）だと言われています。その年のコンペティションで、ディープラーニングによる手法——通称、AlexNet——が、圧倒的な成績で優勝し、これまでの画像認識に対するアプローチを根底から覆しました。まさに、転換点となった2012年のディープラーニングの逆襲により、それ以降のコンペティションでは、常にディープラーニングが主役に躍り出ました。ここでは、ILSVRCという大規模画像認識のコンペティションを軸に、最近のディープラーニングのトレンドを見ていきたいと思います。</p>

<h3 id='h8-2-1'><span class='secno'>8.2.1　</span>ImageNet</h3>
<p><!-- IDX:ImageNet -->ImageNet<u>［25］</u>は、100万枚を超える画像のデータセットです。<a href='./ch08.xhtml#fig08_7'>図8-7</a>に示すように、さまざまな種類の画像が含まれており、それぞれの画像にはラベル（クラス名）が紐付けられています。この巨大なデータセットを使って、ILSVRCという画像認識のコンペティションが毎年行われます。</p>
<div class='image' id='fig08_7'>
<img alt='大規模データセットImageNetのデータ例（文献&lt;u&gt;［25］&lt;/u&gt;より引用）' src='images/ch08/fig08_7.png'/>
<p class='caption'>
図8-7　大規模データセットImageNetのデータ例（文献<u>［25］</u>より引用）
</p>
</div>
<p>ILSVRCのコンペティションには、テスト項目がいくつかありますが、その中のひとつに「クラス分類（classification）」があります（「クラス分類」の部門では、1,000クラスの分類を行って認識精度を競います）。それでは、ここ最近のILSVRCのクラス分類の結果を見てみましょう。<a href='./ch08.xhtml#fig08_8'>図8-8</a>に、ILSVRCのクラス分類を対象として、2010年から最近までの優勝チームの成績を示します。ここでは、上位5クラスまでに正解が入っている場合を「正しい」と見なして、その際の誤認識率を棒グラフで表します。</p>
<div class='image' id='fig08_8'>
<img alt='ILSVRCにおける優秀チームの成績の推移：縦軸は誤認識率、横軸は各年。横軸の括弧内には、チーム名または手法名を示す' src='images/ch08/fig08_8.png'/>
<p class='caption'>
図8-8　ILSVRCにおける優秀チームの成績の推移：縦軸は誤認識率、横軸は各年。横軸の括弧内には、チーム名または手法名を示す
</p>
</div>
<p><a href='./ch08.xhtml#fig08_8'>図8-8</a>のグラフで注目すべき点は、2012年を境にディープラーニングによる手法が常にトップに立っているという点です。実際、2012年のAlexNetが誤認識率を大幅に下げていることが分かります。そして、それ以降、ディープラーニングによる手法が着実に精度を改善してきています。特に、2015年のResNet——150層を超えるディープなネットワーク——においては、誤認識率を3.5%まで下げてきました。ちなみに、この結果は、一般的な人間の認識能力を上回ったとさえ言われています。</p>
<p>さて、ここ数年の素晴らしい成績を残してきたディープラーニングですが、中でもVGG、GoogLeNet、ResNetは有名なネットワークです。これらのネットワークは、ディープラーニングに関連するさまざまな場所で出くわすでしょう。ここでは、これら3つの有名なネットワークを簡単に紹介していきます。</p>

<h3 id='h8-2-2'><span class='secno'>8.2.2　</span>VGG</h3>
<p><!-- IDX:VGG -->VGGは、畳み込み層とプーリング層から構成される“基本的”なCNNです。ただし、<a href='./ch08.xhtml#fig08_9'>図8-9</a>に示すように、重みのある層（畳み込み層や全結合層）を全部で16層（もしくは19層）まで重ねてディープにしている点が特徴です（層の深さに応じて、「VGG16」や「VGG19」と呼ぶ場合があります）。</p>
<div class='image' id='fig08_9'>
<img alt='VGG（文献&lt;u&gt;［22］&lt;/u&gt;を参考に作成）' src='images/ch08/fig08_9.png'/>
<p class='caption'>
図8-9　VGG（文献<u>［22］</u>を参考に作成）
</p>
</div>
<p>VGGで注目すべきポイントは、3×3の小さなフィルターによる畳み込み層を連続して行っている点です。図に示すとおり、畳み込み層を2回から4回連続し、プーリング層でサイズを半分にするという処理を繰り返し行います。そして、最後に全結合層を経由して結果を出力します。</p>
<div class='note'><table class='note'><tr><td class='center top' rowspan='2'><img alt='[注記]' class='noteicon' src='images/note.png'/></td></tr><tr><td>
<p>VGGは、2014年のコンペティションで2位の成績に終わりました（次に紹介するGoogLeNetが、2014年の勝者でした）。性能の面では1位のGoogLeNetには及びませんでしたが、VGGはとてもシンプルな構成であり応用性が高いため、多くの技術者はVGGベースのネットワークを好んで使います。</p>
</td></tr></table></div>

<h3 id='h8-2-3'><span class='secno'>8.2.3　</span>GoogLeNet</h3>
<p><!-- IDX:GoogLeNet -->GoogLeNetのネットワーク構成を<a href='./ch08.xhtml#fig08_10'>図8-10</a>に示します。図中の矩形が、畳み込み層やプーリング層などのレイヤを表しています。</p>
<div class='image' id='fig08_10'>
<img alt='GoogLeNet（文献&lt;u&gt;［23］&lt;/u&gt;より引用）' src='images/ch08/fig08_10.png'/>
<p class='caption'>
図8-10　GoogLeNet（文献<u>［23］</u>より引用）
</p>
</div>
<p>図を見るかぎり、とても複雑そうに見えるネットワーク構成ですが、基本的にはこれまで見てきたCNNと同じ構成です。ただし、GoogLeNetは、ネットワークが縦方向の深さだけではなく、横方向にも深さ（広がり）を持っているという点が特徴です。</p>
<p>GoogLeNetには横方向に“幅”があります。これは、「<!-- IDX:インセプション構造 -->インセプション構造」と呼ばれ、<a href='./ch08.xhtml#fig08_11'>図8-11</a>で示す構造をベースとします。</p>
<div class='image' id='fig08_11'>
<img alt='GoogLeNetのインセプション構造（文献&lt;u&gt;［23］&lt;/u&gt;より引用）' src='images/ch08/fig08_11.png'/>
<p class='caption'>
図8-11　GoogLeNetのインセプション構造（文献<u>［23］</u>より引用）
</p>
</div>
<p>インセプション構造は、<a href='./ch08.xhtml#fig08_11'>図8-11</a>に示すように、サイズの異なるフィルター（とプーリング）を複数適用し、その結果を結合します。このインセプション構造をひとつのビルディングブロック（構成要素）として使用するのが、GoogLeNetの特徴です。また、GoogLeNetでは、サイズが1×1のフィルターの畳み込み層を多くの場所で使用します。この1×1の畳み込み演算はチャンネル方向にサイズを減らすことで、パラメータの削減や処理の高速化に貢献できます（詳しくは原著論文<u>［23］</u>を参照してください）。</p>

<h3 id='h8-2-4'><span class='secno'>8.2.4　</span>ResNet</h3>
<p><!-- IDX:ResNet -->ResNet<u>［24］</u>はMicrosoftのチームによって開発されたネットワークです。その特徴は、これまで以上に層を深くできるような“仕掛け”にあります。</p>
<p>これまで、層を深くすることが性能の向上において重要であることは分かっていました。しかし、ディープラーニングの学習においては、層を深くしすぎると、学習がうまくいかず、最終的な性能が劣ることも多々ありました。ResNetでは、そのような問題を解決するために「<!-- IDX:スキップ構造 -->スキップ構造」（「ショートカット」や「バイパス」とも呼ぶ）を導入します。このスキップ構造を導入することで、層を深くすることに比例して、性能を向上させることができるようになりました（もちろん、層を深くすることに限界はありますが）。</p>
<p>スキップ構造とは、<a href='./ch08.xhtml#fig08_12'>図8-12</a>に示すように、入力データの畳み込み層をまたいで——スキップして——出力に合算する構造を言います。</p>
<div class='image' id='fig08_12'>
<img alt='ResNetの構成要素（文献&lt;u&gt;［24］&lt;/u&gt;より引用）：ここで「weight layer」とは畳み込み層を指す' src='images/ch08/fig08_12.png'/>
<p class='caption'>
図8-12　ResNetの構成要素（文献<u>［24］</u>より引用）：ここで「weight layer」とは畳み込み層を指す
</p>
</div>
<p><a href='./ch08.xhtml#fig08_12'>図8-12</a>では、2層連続する畳み込み層において、入力の<span class='equation mathimage'><img alt='\mathbf{x}' src='images/math/a0ee1a37faae1abbf3cd71e8af5afa5b.png' style='height: 0.5em'/></span>を2層先の出力にスキップしてつなげます。本来であれば、2層の畳み込み層の出力が<span class='equation mathimage'><img alt='\mathcal{F}(\mathbf{x})' src='images/math/e646df6940281f0199c9890eee268586.png' style='height: 1.16em'/></span>であるところを、スキップ構造によって<span class='equation mathimage'><img alt='\mathcal{F}(\mathbf{x}) + \mathbf{x}' src='images/math/878c57983c0da7c44504a6d8fb00ee21.png' style='height: 1.16em'/></span>にする点がポイントです。このようなスキップ構造を取り入れることで、層を深くしても効率良く学習することができます。これは、逆伝播の際に、スキップ構造によって信号が減衰することなく伝わっていくからです。</p>
<div class='note'><table class='note'><tr><td class='center top' rowspan='2'><img alt='[注記]' class='noteicon' src='images/note.png'/></td></tr><tr><td>
<p>スキップ構造は入力データを“そのまま”流すだけなので、逆伝播時も、上流からの勾配を“そのまま”下流へ流します。ここでのポイントは、上流からの勾配に対して何の手も加えずに、“そのまま”流すということです。そのため、スキップ構造によって、勾配が小さくなったり（または大きくなりすぎたり）する心配がなく、前層のレイヤに「意味のある勾配」が伝わっていくことが期待できます。これまであった、層を深くすることで勾配が小さくなる勾配消失問題は、このスキップ構造で軽減することが期待できます。</p>
</td></tr></table></div>
<p>ResNetは、先に説明したVGGのネットワークをベースとして、スキップ構造を取り入れ、層を深くしています。その結果は、<a href='./ch08.xhtml#fig08_13'>図8-13</a>のようになります。</p>
<div class='image' id='fig08_13'>
<img alt='ResNet（文献&lt;u&gt;［24］&lt;/u&gt;より引用）：ブロックが3×3の畳み込み層に対応。層をまたぐスキップ構造が特徴' src='images/ch08/fig08_13.png'/>
<p class='caption'>
図8-13　ResNet（文献<u>［24］</u>より引用）：ブロックが3×3の畳み込み層に対応。層をまたぐスキップ構造が特徴
</p>
</div>
<p><a href='./ch08.xhtml#fig08_13'>図8-13</a>に示すように、ResNetは、畳み込み層を2層おきにスキップしてつなぎ、層を深くしていきます。なお、実験によって、150層以上に深くしても認識精度は向上し続けることが分かりました。そして、ILSVRCのコンペティションでは、誤認識率（上位5クラス以内に正解が含まれる精度の誤認識率）が3.5%という、驚異的な結果を出しました。</p>
<div class='note'><table class='note'><tr><td class='center top' rowspan='2'><img alt='[注記]' class='noteicon' src='images/note.png'/></td></tr><tr><td>
<p>ImageNetの巨大なデータセットを使って学習した重みデータを有効活用するということが実践的によく行われます。これは<!-- IDX:転移学習 --><b>転移学習</b>と言って、学習済みの重み（の一部）を別のニューラルネットワークにコピーして、再学習を行います。たとえば、VGGと同じ構成のネットワークを用意し、学習済みの重みを初期値とし、新しいデータセットを対象に、<!-- IDX:再学習 -->再学習（<!-- IDX:fine tuning -->fine tuning）を行います。転移学習は、手元にあるデータセットが少ない場合において、特に有効な手法です。</p>
</td></tr></table></div>

<h2 id='h8-3'><span class='secno'>8.3　</span>ディープラーニングの高速化</h2>
<p><!-- IDX:高速化 -->ビッグデータとネットワークの大規模化により、ディープラーニングでは大量の演算を行う必要があります。これまで私たちはCPUを使って計算を行ってきましたが、CPUだけでディープラーニングに立ち向かうのは心許ないというのが現実です。実際、周りを見渡してみると、ディープラーニングのフレームワークの多くはGPU（Graphics Processing Unit）をサポートしており、大量の演算を高速に処理することが可能です。また最近のフレームワークでは、複数のGPUや複数台のマシンでの分散学習にも対応し始めています。ここでは、ディープラーニングにおける計算の高速化にスポットを当て、話を進めていきます。なお、私たちのディープラーニングの実装は8.1節で終わりにして、ここで説明するような高速化（GPU対応など）は行わないものとします。</p>

<h3 id='h8-3-1'><span class='secno'>8.3.1　</span>取り組むべき問題</h3>
<p>ディープラーニングの高速化の話を進める前に、ディープラーニングでは、どういった処理に時間が費やされているのかを見てみましょう。<a href='./ch08.xhtml#fig08_14'>図8-14</a>には、AlexNetの<code class='tt'>forward</code>処理を対象に、各層に費やされる時間を円グラフで示します。</p>
<div class='image' id='fig08_14'>
<img alt='AlexNetのforward処理における各層の時間比率：左がGPU、右がCPUを使用した場合。図中の「conv」は畳み込み層、「pool」はプーリング層、「fc」は全結合層、「norm」は正規化層に対応する（文献&lt;u&gt;［26］&lt;/u&gt;より引用）' src='images/ch08/fig08_14.png'/>
<p class='caption'>
図8-14　AlexNetのforward処理における各層の時間比率：左がGPU、右がCPUを使用した場合。図中の「conv」は畳み込み層、「pool」はプーリング層、「fc」は全結合層、「norm」は正規化層に対応する（文献<u>［26］</u>より引用）
</p>
</div>
<p>図から分かるとおり、AlexNetでは多くの時間が畳み込み層に費やされます。実際、畳み込み層の処理時間を合計すると、GPUでは全体の95%、CPUでは全体の89%までに達するのです！ そのため、畳み込み層で行われる演算をいかに高速に効率良く行うかという点がディープラーニングでの課題になります。また、<a href='./ch08.xhtml#fig08_14'>図8-14</a>の結果は、推論時のものですが、学習時も同様に、畳み込み層で多くの時間が費やされることになります。</p>
<div class='note'><table class='note'><tr><td class='center top' rowspan='2'><img alt='[注記]' class='noteicon' src='images/note.png'/></td></tr><tr><td>
<p>畳み込み層で行う演算は、<a href='ch07.xhtml#h7-2'>「7.2 畳み込み層」</a>で説明したように、元をたどると「積和演算」に行き着きます。そのため、ディープラーニングの高速化の主題は、大量の「積和演算」をいかに高速に効率良く計算するかということになるのです。</p>
</td></tr></table></div>

<h3 id='h8-3-2'><span class='secno'>8.3.2　</span>GPUによる高速化</h3>
<p><!-- IDX:GPU -->GPUは元々、グラフィックのための専用のボードとして利用されてきました。しかし最近では、グラフィック処理だけでなく、汎用的な数値計算にもGPUは利用されます。GPUは並列的な数値演算を高速に行うことができるため、その圧倒的なパワーをさまざまな用途に活用しようというのが<!-- IDX:GPUコンピューティング --><b>GPUコンピューティング</b>の狙いです。なお、GPUによって汎用的な数値演算を行うことを、GPUコンピューティングと言います。</p>
<p>ディープラーニングでは、大量の積和演算（または、大きな行列の内積）を行う必要があります。そのような大量にある並列的な演算は、GPUが得意とするところです（逆に、CPUは連続的で複雑な計算を得意とします）。そのため、ディープラーニングの演算では、GPUを利用することによって、CPU単体の場合に比べて驚くほどの高速化を達成することができます。それでは、GPUによってどれくらいの高速化が達成できるのか例を見てみましょう。次の<a href='./ch08.xhtml#fig08_15'>図8-15</a>は、AlexNetの学習に要する時間を、CPUとGPUで比較した結果です。</p>
<div class='image' id='fig08_15'>
<img alt='AlexNetの学習に要する時間を、CPUの「16-core Xeon CPU」とGPUの「Titanシリーズ」で比較した結果（文献&lt;u&gt;［27］&lt;/u&gt;より引用）' src='images/ch08/fig08_15.png'/>
<p class='caption'>
図8-15　AlexNetの学習に要する時間を、CPUの「16-core Xeon CPU」とGPUの「Titanシリーズ」で比較した結果（文献<u>［27］</u>より引用）
</p>
</div>
<p>図から分かるとおり、CPUでは40日以上も要する時間を、GPUによって6日まで短縮できる結果となっています。また。<!-- IDX:cuDNN -->cuDNNというディープラーニングに最適化されたライブラリを用いることで、さらなる高速化を達成できることを図は示しています。</p>
<p>ところで、GPUは主に、NVIDIA社とAMD社の2社によって提供されています。両者のGPUは汎用的な数値演算のために利用することはできますが、ディープラーニングと“親しい”のはNVIDIAのGPUです。実際、多くのディープラーニングのフレームワークでは、NVIDIAのGPUだけから恩恵を受けることができます。これは、NVIDIAが提供する<!-- IDX:CUDA -->CUDAというGPUコンピューティング向けの統合開発環境が、ディープラーニングのフレームワークで使われているからです。<a href='./ch08.xhtml#fig08_15'>図8-15</a>に登場するcuDNNは、CUDAの上で動作するライブラリで、これにはディープラーニング用に最適化された関数などが実装されています。</p>
<div class='note'><table class='note'><tr><td class='center top' rowspan='2'><img alt='[注記]' class='noteicon' src='images/note.png'/></td></tr><tr><td>
<p>畳み込み層で行う演算は、im2colによって、大きな行列の内積に変換することができました。このim2col方式の実装は、GPUにとって都合の良い実装方式です。というのは、GPUは“ちまちま”と小さな単位で計算するよりも、大きなまとまりを一気に計算することを得意とするからです。つまり、im2colによって巨大な行列の内積としてまとめて計算することで、GPUの本領を発揮させることが容易になるのです。</p>
</td></tr></table></div>

<h3 id='h8-3-3'><span class='secno'>8.3.3　</span>分散学習</h3>
<p><!-- IDX:分散学習 -->GPUによりディープラーニングの演算はかなり高速化できますが、それでもディープなネットワークになると、学習のために数日・数週間といったオーダーの時間が必要になります。そして、これまで見てきたように、ディープラーニングでは多くの試行錯誤を伴います。良いネットワークを作るためには、さまざまなことを数多く試す必要があり、そのためには、1回の学習に要する時間をできるだけ小さくしたいという要望が必然的に生まれます。そこで、ディープラーニングの学習をスケールアウトさせようという考え——つまり、「分散学習」——が重要になってくるのです。</p>
<p>ディープラーニングに必要な計算をさらに高速化するためには、複数のGPUや複数台のマシンで分散して計算を行うことが考えられます。現在、ディープラーニングのフレームワークでは、複数GPUや複数マシンによる分散学習をサポートしたものがいくつか現れてきています。中でもGoogleの<!-- IDX:TensorFlow -->TensorFlowやMicrosoftの<!-- IDX:CNTK -->CNTK（Computational Network Toolkit）は、分散学習を重要視して開発が行われています。巨大なデータセンターの低遅延・高スループットなネットワークを支えとして、それらのフレームワークによる分散学習は驚くほどの効果を見せています。</p>
<p>分散学習によって、どれだけ高速化を達成できるのでしょうか？ <a href='./ch08.xhtml#fig08_16'>図8-16</a>には、TensorFlowによる分散学習の効果が示されています。</p>
<div class='image' id='fig08_16'>
<img alt='TensorFlowによる分散学習の効果：横軸はGPUの個数、縦軸はGPUひとつのときと比べた高速化率（文献&lt;u&gt;［28］&lt;/u&gt;より引用）' src='images/ch08/fig08_16.png'/>
<p class='caption'>
図8-16　TensorFlowによる分散学習の効果：横軸はGPUの個数、縦軸はGPUひとつのときと比べた高速化率（文献<u>［28］</u>より引用）
</p>
</div>
<p><a href='./ch08.xhtml#fig08_16'>図8-16</a>に示すように、使用するGPUが増えるに従って、学習速度も向上することが分かります。実際、100個のGPU（複数のマシンに設定された合計100個のGPU）によって、GPUがひとつのときに比べて56倍の高速化が可能なようです！ これは、たとえば、7日かかっていた学習がわずか3時間で完了することを意味し、分散学習の驚くべき効果を物語っています。</p>
<p>分散学習について「どのように計算を分散させるか」というテーマは、とても難しい問題です。マシン間での通信やデータの同期など、簡単には解決できない問題をいくつもはらんでいます。そのような難しい問題は、TensorFlowのような優れたフレームワークに任せるのがよいでしょう。ここでは、分散学習の詳細には立ち入らないことにします。分散学習の技術的な内容についてはTensorFlowの技術論文（ホワイトペーパー）<u>［29］</u>などを参照してください。</p>

<h3 id='h8-3-4'><span class='secno'>8.3.4　</span>演算精度のビット削減</h3>
<p><!-- IDX:演算精度 -->ディープラーニングの高速化においては、計算量に加えて、メモリ容量やバス帯域などがボトルネックになりえます。メモリ容量の点で言うと、大量の重みパラメータや中間データをメモリに収めることを考慮する必要があります。また、バス帯域の点では、GPU（もしくはCPU）のバスを流れるデータが増加してある制限を超えると、そこがボトルネックになります。このようなケースを想定すると、ネットワークを流れるデータのビット数は、できるだけ小さくすることが望まれます。</p>
<p>コンピュータでは実数を表現するために、主に64ビットや32ビットの浮動小数点数が使われます。数を表現するために多くのビットを使うことで、数値計算時の誤差による影響は少なくなりますが、その分、計算の処理コストやメモリ使用量が増大し、バス帯域に負荷をかけます。</p>
<p>数値精度（何ビットのデータで数値を表現するかということ）に関して、ディープラーニングで分かっていることは、ディープラーニングでは数値精度のビット数をそこまで必要としない、ということです。これは、ニューラルネットワークの重要な性質のひとつです。この性質は、ニューラルネットワークのロバスト性によるものです。ここで言うロバスト性とは、たとえば、ニューラルネットワークは、入力画像に小さなノイズがのってしまっても、出力結果が変わらないような頑健な性質があるということです。そのようなロバスト性のおかげで、ネットワークを流れるデータを“劣化”させても、出力結果に与える影響は少ないと考えることができます。</p>
<p>コンピュータ上で小数を表現するには、32ビットの単精度浮動小数点数や64ビットの倍精度浮動小数点数などのフォーマットがありますが、これまでの実験によって、ディープラーニングにおいては、16ビットの<!-- IDX:半精度浮動小数点数 --><b>半精度浮動小数点数</b>（<!-- IDX:half float -->half float）でも、問題なく学習ができることが分かっています<u>［30］</u>。実際、NVIDIAの次世代GPUであるPascalアーキテクチャでは、半精度浮動小数点数の演算もサポートされるため、これからは半精度浮動小数点数が標準的に用いられると考えられます。</p>
<div class='note'><table class='note'><tr><td class='center top' rowspan='2'><img alt='[注記]' class='noteicon' src='images/note.png'/></td></tr><tr><td>
<p>NVIDIAのMaxwell世代のGPUは、半精度浮動小数点数はストレージ（データを保持する機能）としてはサポートしていましたが、演算自体は16ビットでは行っていませんでした。次世代のPascalアーキテクチャは、演算も含めて16ビットで行うため、単に半精度浮動小数点数で計算を行うだけで、前世代のGPUと比較しておよそ2倍の高速化が期待できます。</p>
</td></tr></table></div>
<p>なお、これまでのディープラーニングの実装では数値精度に注意を払いませんでしたが、Pythonでは一般的に64ビットの浮動小数点数が使われます。NumPyには16ビットの半精度浮動小数点数の型が用意されています（ただし、ストレージとして16ビットの型があるだけで、演算自体は16ビットでは行われません）。NumPyの半精度浮動小数点数を使っても認識精度が低下しないことは簡単に示すことができます。興味のある方は、<code class='tt'>ch08/half_float_network.py</code>を参照してください。</p>
<p>ディープラーニングのビット数を削減するというテーマの研究は、これまでにも、いくつか行われています。最近では、重みや中間データを1ビットで表現する「Binarized Neural Networks」という手法が提案されています<u>［31］</u>。ディープラーニングの高速化のためにビットを削減するというテーマは、今後目が離せない分野であり、特に組み込み向けでディープラーニングを利用する際に重要なテーマとなってきます。</p>

<h2 id='h8-4'><span class='secno'>8.4　</span>ディープラーニングの実用例</h2>
<p>これまでディープラーニングを使った例として、手書き数字認識のような画像のクラス分類——これを「物体認識」と言います——を中心に見てきました。しかし、ディープラーニングは、物体認識だけではなく、さまざまな問題に適用することができます。また、画像や音声、自然言語など、分野は異なりますが、多くの問題に対して、ディープラーニングは優れた性能を発揮します。ここではディープラーニングができること（アプリケーション）について、コンピュータビジョンの分野を中心に、いくつか紹介したいと思います。</p>

<h3 id='h8-4-1'><span class='secno'>8.4.1　</span>物体検出</h3>
<p><!-- IDX:物体検出 -->物体検出は、画像中から物体の位置の特定を含めてクラス分類を行う問題です。<a href='./ch08.xhtml#fig08_17'>図8-17</a>に示すように、画像中から物体の種類と物体の位置を特定します。</p>
<div class='image' id='fig08_17'>
<img alt='物体検出の例（文献&lt;u&gt;［34］&lt;/u&gt;より引用）' src='images/ch08/fig08_17.png'/>
<p class='caption'>
図8-17　物体検出の例（文献<u>［34］</u>より引用）
</p>
</div>
<p><a href='./ch08.xhtml#fig08_17'>図8-17</a>を見て分かるとおり、物体検出は、物体認識よりも難しい問題です。これまで見てきた物体認識は、画像全体を対象にしましたが、物体検出では、画像中からクラスの位置まで特定する必要があります。しかも、物体は複数存在する可能性もあります。</p>
<p>このような物体検出の問題に対して、CNNをベースとした手法がいくつか提案されています。それらの手法は、とても優れた性能を示しており、物体検出の問題に対しても、ディープラーニングが有効であることを物語っています。</p>
<p>さて、CNNを用いて物体検出を行う手法はいくつかありますが、その中でも<!-- IDX:R&#45;CNN -->R-CNN<u>［35］</u>と呼ばれる手法が有名です。<a href='./ch08.xhtml#fig08_18'>図8-18</a>にR-CNNの処理フローを示します。</p>
<div class='image' id='fig08_18'>
<img alt='R-CNNの処理フロー（文献&lt;u&gt;［35］&lt;/u&gt;より引用）' src='images/ch08/fig08_18.png'/>
<p class='caption'>
図8-18　R-CNNの処理フロー（文献<u>［35］</u>より引用）
</p>
</div>
<p><a href='./ch08.xhtml#fig08_18'>図8-18</a>で注目してほしい点は、図中の「2. 候補領域抽出（Extract region proposals）」と「3. CNN特徴の計算（Compute CNN features）」の処理パートです。これは、最初にオブジェクトらしい領域を（何らかの方法で）探し出し、そして、その抽出された領域に対してCNNを適用しクラス分類を行っています。R-CNNでは、画像を正方形に変形したり、分類の際にSVM（サポートベクターマシン）を使ったりと、実際の処理フローはやや複雑ですが、大きな視点では、先の2つの処理——候補領域抽出とCNN——によって構成されています。</p>
<p>なお、R-CNNの前半の処理である「候補領域抽出（オブジェクトらしい物体を見つける処理）」には、コンピュータビジョンで培われたさまざまな手法を用いることができます。R-CNNの論文では、Selective Searchと呼ばれる手法が使われています。最近では、この候補領域抽出までもCNNによって行う「<!-- IDX:Faster R&#45;CNN -->Faster R-CNN」<u>［36］</u>という手法が提案されています。Faster R-CNNは、すべての処理をひとつのCNNで行うので、高速な処理が可能になります。</p>

<h3 id='h8-4-2'><span class='secno'>8.4.2　</span>セグメンテーション</h3>
<p><!-- IDX:セグメンテーション -->セグメンテーションとは、画像に対してピクセルレベルでクラス分類を行う問題です。<a href='./ch08.xhtml#fig08_19'>図8-19</a>に示すように、ピクセル単位でオブジェクトごとに色付けされた教師データを使って学習を行います。そして、推論の際には、入力画像のすべてのピクセルに対して、クラス分類を行います。</p>
<div class='image' id='fig08_19'>
<img alt='セグメンテーションの例（文献&lt;u&gt;［34］&lt;/u&gt;より引用）：左が入力画像、右が教師用のラベリング画像' src='images/ch08/fig08_19.png'/>
<p class='caption'>
図8-19　セグメンテーションの例（文献<u>［34］</u>より引用）：左が入力画像、右が教師用のラベリング画像
</p>
</div>
<p>さて、これまで実装してきたニューラルネットワークは、画像全体に対してクラス分類を行ってきました。これをピクセルレベルに落とし込むには、どのようにすればよいでしょうか？</p>
<p>ニューラルネットワークによって、セグメンテーションを行う最も単純な方法は、すべてのピクセルを対象として、ピクセルごとに推論処理を行うことでしょう。たとえば、ある矩形領域の中心のピクセルに対してクラス分類を行うネットワークを用意して、すべてのピクセルを対象に推論処理を実行するのです。お察しのとおり、そのような方法ではピクセルの数だけ<code class='tt'>forward</code>処理を行う必要があり、多くの時間が必要になってしまいます（正確には、畳み込み演算で、多くの領域を再計算するという無駄な計算が発生してしまうことが問題になります）。そのような計算の無駄を改善する方法として、<!-- IDX:FCN -->FCN（Fully Convolutional Network）<u>［37］</u>という手法が提案されています。これは、1回の<code class='tt'>forward</code>処理ですべてのピクセルに対してクラス分類を行います（<a href='./ch08.xhtml#fig08_20'>図8-20</a>参照）。</p>
<div class='image' id='fig08_20'>
<img alt='FCNの全体図（文献&lt;u&gt;［37］&lt;/u&gt;より引用）' src='images/ch08/fig08_20.png'/>
<p class='caption'>
図8-20　FCNの全体図（文献<u>［37］</u>より引用）
</p>
</div>
<p>FCNのFully Convolutional Networkを直訳すれば、「すべてが畳み込み層から構成されるネットワーク」という意味です。これは、一般的なCNNが全結合層を含むのに対して、FCNでは、全結合層を「同じ働きをする畳み込み層」に置き換えます。物体認識で用いたネットワークの全結合層では、中間データの空間ボリュームは1列に並んだノードとして処理されていましたが、畳み込み層だけから構成されるネットワークでは、空間ボリュームは保たれたまま最後の出力まで処理することができます。</p>
<p>また、FCNの特徴としては、<a href='./ch08.xhtml#fig08_20'>図8-20</a>に示すように、最後に空間サイズを拡大する処理を導入している点です。この拡大処理によって、小さくなった中間データを入力画像のサイズと同じ大きさまで一気に拡大することができるのです。なお、FCNの最後に行う拡大処理は、バイリニア補間による拡大（バイリニア拡大）です。FCNでは、このバイリニア拡大を<!-- IDX:デコンボリューション -->デコンボリューション（逆畳み込み演算）によって実現しています（詳細はFCNの論文<u>［37］</u>を参照）。</p>
<div class='note'><table class='note'><tr><td class='center top' rowspan='2'><img alt='[注記]' class='noteicon' src='images/note.png'/></td></tr><tr><td>
<p>全結合層では、出力がすべての入力と結びつきます。これとまったく同じ構成の結びつきを畳み込み層によっても実現することができます。たとえば、入力サイズが32×10×10（チャンネル数が32、高さ10、横幅10）のデータに対する全結合層は、32×10×10のフィルターサイズの畳み込み層に置き換えることができます。もし、全結合層の出力ノード数が100であれば、畳み込み層では、先の32×10×10のフィルターを100個用意すれば、完全に同じ処理を実現することができます。このように、全結合層は、同等の処理を行う畳み込み層に置き換えることができるのです。</p>
</td></tr></table></div>

<h3 id='h8-4-3'><span class='secno'>8.4.3　</span>画像キャプション生成</h3>
<p><!-- IDX:画像キャプション生成 -->コンピュータビジョンと自然言語を融合したおもしろい研究があります。<a href='./ch08.xhtml#fig08_21'>図8-21</a>に示すように、画像を与えると、その画像を説明する文章（画像キャプション）を自動で生成する研究です。</p>
<div class='image' id='fig08_21'>
<img alt='ディープラーニングによる画像のキャプション生成の例（文献&lt;u&gt;［38］&lt;/u&gt;より引用）' src='images/ch08/fig08_21.png'/>
<p class='caption'>
図8-21　ディープラーニングによる画像のキャプション生成の例（文献<u>［38］</u>より引用）
</p>
</div>
<p>画像を与えると、その画像の内容を表するテキストが<a href='./ch08.xhtml#fig08_21'>図8-21</a>のように自動で生成されます。たとえば、一番左上の画像は、「未舗装の道路上でバイクに乗る人（A person riding a motorcycle on a dirt road.）」という文章です（この文章を画像だけから自動で生成します）。確かに、テキストの内容と画像は一致しています。しかも、バイクに乗っていることだけでなく、未舗装の荒れた道路であることまで“理解”しているとは驚きです。</p>
<p>ディープラーニングによって、画像キャプションを生成する代表的な方法に、<!-- IDX:NIC --><b>NIC</b>（Neural Image Caption）と呼ばれるモデルがあります。NICは<a href='./ch08.xhtml#fig08_22'>図8-22</a>に示すように、ディープなCNNと自然言語を扱うための<!-- IDX:RNN --><b>RNN</b>（Recurrent Neural Network）から構成されます。RNNとは、再帰的なつながりを持つネットワークであり、自然言語や時系列データなどの連続性のあるデータに対してよく用いられます。</p>
<div class='image' id='fig08_22'>
<img alt='Neural Image Caption（NIC）の全体構成（文献&lt;u&gt;［38］&lt;/u&gt;より引用）' src='images/ch08/fig08_22.png'/>
<p class='caption'>
図8-22　Neural Image Caption（NIC）の全体構成（文献<u>［38］</u>より引用）
</p>
</div>
<p>NICは、画像からCNNによって特徴を抽出し、その特徴をRNNに渡します。RNNは、CNNが抽出した特徴を初期値として、テキストを“再帰的”に生成していきます。ここでは、これ以上、技術の詳細には立ち入らないことにしますが、基本的には、NICは2つのニューラルネットワーク——CNNとRNN——を組み合わせたシンプルな構成です。それによって、驚くほど高精度な画像キャプションの生成が行えるのです。なお、画像と自然言語といったような、複数の種類の情報を組み合わせて処理することを<b>マルチモーダル処理</b>と言います。マルチモーダル処理は、近年注目を集めている分野です。</p>
<div class='note'><table class='note'><tr><td class='center top' rowspan='2'><img alt='[注記]' class='noteicon' src='images/note.png'/></td></tr><tr><td>
<p>RNNのRはRecurrent（再帰的な）を表します。この再帰とは、ニューラルネットワークの再帰的なネットワーク構造を指します。この再帰的な構造によって、以前に生成した情報から影響を受ける——言い換えると、過去の情報を記憶する——点がRNNの特徴です。たとえば「私」という言葉を生成した後に、次に生成する言葉は「私」という言葉の影響を受けて、「は」という言葉を生成する。そして、「私は」というこれまで生成した言葉から影響を受けて、次に「寝る」という言葉を生成する、といったことが行われます。自然言語や時系列データなど、連続性のあるデータに対して、RNNは過去の情報を記憶するように動作します。</p>
</td></tr></table></div>

<h2 id='h8-5'><span class='secno'>8.5　</span>ディープラーニングの未来</h2>
<p>ディープラーニングは、従来の分野にとどまらず、さまざまな分野で用いられるようになってきました。ここでは、ディープラーニングの可能性と、これからの未来を感じさせるような研究をいくつか紹介したいと思います。</p>

<h3 id='h8-5-1'><span class='secno'>8.5.1　</span>画像スタイル変換</h3>
<p><!-- IDX:画像スタイル変換 -->ディープラーニングを使って、アーティストのような絵を“描かせる”という研究があります。次の<a href='./ch08.xhtml#fig08_23'>図8-23</a>で示す例は、2つの画像を入力し、新しい画像を生成するという研究です。2つの画像のうちひとつは「コンテンツ画像」、もうひとつは「スタイル画像」と呼び、その2枚の画像を入力すると、新しい画像が生成されます。</p>
<div class='image' id='fig08_23'>
<img alt='論文「A Neural Algorithm of Artistic Style」による画像スタイル変換の例：左上が「スタイル画像」、右上が「コンテンツ画像」、下の画像が新たに生成された画像（画像は文献&lt;u&gt;［40］&lt;/u&gt;より引用）' src='images/ch08/fig08_23.png'/>
<p class='caption'>
図8-23　論文「A Neural Algorithm of Artistic Style」による画像スタイル変換の例：左上が「スタイル画像」、右上が「コンテンツ画像」、下の画像が新たに生成された画像（画像は文献<u>［40］</u>より引用）
</p>
</div>
<p><a href='./ch08.xhtml#fig08_23'>図8-23</a>に示すように、ゴッホの描画スタイルを、コンテンツ画像に適用するように指定すれば、ディープラーニングが、指定されたとおりに新しい絵画を描いてくれます。これは「A Neural Algorithm of Artistic Style」<u>［39］</u>という論文の研究で、発表されるやいなや世界中で多くの注目を集めました。</p>
<p>ここでは、この研究の詳細な説明は行いません。技術の大枠だけを述べるとすれば、先の手法では、ネットワークの中間データが「コンテンツ画像」の中間データに近づくように学習します。そうすることで、入力画像をコンテンツ画像の形状に似せることができます。また、「スタイル画像」からスタイルを吸収するために、スタイル行列という概念を導入します。そのスタイル行列のズレを小さくするように学習することで、入力画像をゴッホのスタイルに近づけるといったことが実現できるのです。</p>

<h3 id='h8-5-2'><span class='secno'>8.5.2　</span>画像生成</h3>
<p><!-- IDX:画像生成 -->先の画像スタイル変換の例は、新しい画像を生成する際に2枚の画像を入力しました。一方そのような研究とは別に、新しい画像を生成する際に、何の画像も必要とせずに新たな画像を描き出すといった研究も行われています（先に大量の画像を使って学習を行いますが、新しい画像を“描く”際には何の画像も必要としません）。たとえば、「ベッドルーム」の画像をゼロから生成するといったことがディープラーニングによって実現できます。<a href='./ch08.xhtml#fig08_24'>図8-24</a>で示す画像は、<!-- IDX:DCGAN --><b>DCGAN</b>（Deep Convolutional Generative Adversarial Network）<u>［41］</u>という手法によって生成されたベッドルームの画像例です。</p>
<div class='image' id='fig08_24'>
<img alt='DCGANによって新たに生成されたベッドルームの画像（文献&lt;u&gt;［41］&lt;/u&gt;より引用）' src='images/ch08/fig08_24.png'/>
<p class='caption'>
図8-24　DCGANによって新たに生成されたベッドルームの画像（文献<u>［41］</u>より引用）
</p>
</div>
<p><a href='./ch08.xhtml#fig08_24'>図8-24</a>の画像は本物の写真のように見えるかもしれませんが、これらの画像は、DCGANによって新たに生成された画像です。つまり、DCGANが描き出す画像は、まだ誰も見たことがない画像（学習データには存在しない画像）であり、ゼロから新たに生成された画像なのです。</p>
<p>さて、本物と見間違うほどのクオリティーで画像を描き出すDCGANですが、DCGANは画像が生成される過程をモデル化します。そのモデルを、大量の画像（たとえば、ベッドルームが写っている大量の画像）を使って学習を行います。学習が終われば、後はそのモデルを利用して、新しい画像を生成させることができるのです。</p>
<p>DCGANの中ではディープラーニングが使われています。DCGANの技術の要点は、Generator（生成する人）とDiscriminator（識別する人）と呼ばれる2つのニューラルネットワークを利用している点です。Generatorが本物そっくりの画像を生成し、Discriminatorは、それが本物かどうか——Generatorが生成した画像か、それとも実際に撮影された画像か——を判定します。そのようにして、両者を競わせるように学習させていくことで、Generatorは、より精巧な騙し画像の技術を学習し、Discriminatorは、より高精度に見破ることができる鑑定師のように成長していくのです。両者は互いに切磋琢磨しながら成長していくという点が、<!-- IDX:GAN --><b>GAN</b>（Generative Adversarial Network）と呼ばれる技術のおもしろいところです 。そのように切磋琢磨して成長したGeneratorは、最終的には本物と見間違うほどの画像を描き出せる能力を身につけるのです（もしくは、そのように成長する場合があるのです）。</p>
<div class='note'><table class='note'><tr><td class='center top' rowspan='2'><img alt='[注記]' class='noteicon' src='images/note.png'/></td></tr><tr><td>
<p>これまで見てきた機械学習の問題は、<!-- IDX:教師あり学習 --><b>教師あり学習</b>（supervised learning）と呼ばれるタイプの問題でした。それは、手書き数字認識のように、画像データと教師ラベルが対になって与えられたデータセットを利用します。しかし、ここで取り上げた問題は、教師データは与えられず、単に大量の画像（画像の集合）だけが与えられます。これは、<!-- IDX:教師なし学習 --><b>教師なし学習</b>（unsupervised learning）と呼ばれる問題です。教師なし学習は比較的昔から研究されてきた分野ですが（<!-- IDX:Deep Belief Network --><b>Deep Belief Network</b>や<!-- IDX:Deep Boltzmann Machine --><b>Deep Boltzmann Machine</b>などが有名）、最近では、あまり活発に研究は行われていない印象です。今後、ディープラーニングを使ったDCGANなどのような手法が注目を集めるに従って、教師なし学習のさらなる発展が期待できるかもしれません。</p>
</td></tr></table></div>

<h3 id='h8-5-3'><span class='secno'>8.5.3　</span>自動運転</h3>
<p><!-- IDX:自動運転 -->人間に代わりコンピュータが自動車を運転する「自動運転」の技術が現実味を帯びてきました。自動車メーカーだけでなくIT企業や大学・研究機関なども含めて、自動運転の実現に向けてしのぎを削っています。自動運転は、さまざまな技術——通行ルートを決めるパスプラン（path plan）技術やカメラやレーザーなどのセンシング技術など——が力を合わせて初めて実現することができますが、その中でも周囲の環境を正しく認識する技術が重要な問題だと言われています。これは、日々刻々と変わる環境や縦横無尽に行き交う車や人々を正しく認識することが、非常に難しい問題だからです。</p>
<p>さまざまな環境でもロバストに走行領域を正しく認識できるようになれば、自動運転の実現もそう遠くないかもしれません。そして、最近では、そのような周囲の環境を認識する技術に、ディープラーニングのパワーが期待されています。たとえば、<!-- IDX:SegNet -->SegNet<u>［42］</u>と呼ばれるCNNベースのネットワークは、<a href='./ch08.xhtml#fig08_25'>図8-25</a>に示すように、高精度に走路環境を認識することができます。</p>
<div class='image' id='fig08_25'>
<img alt='ディープラーニングによる画像のセグメンテーションの例：道路や車、建物や歩道などが高精度に認識されている（文献&lt;u&gt;［43］&lt;/u&gt;より引用）' src='images/ch08/fig08_25.png'/>
<p class='caption'>
図8-25　ディープラーニングによる画像のセグメンテーションの例：道路や車、建物や歩道などが高精度に認識されている（文献<u>［43］</u>より引用）
</p>
</div>
<p><a href='./ch08.xhtml#fig08_25'>図8-25</a>に示すように、入力画像に対してセグメンテーション（ピクセルレベルの判定）を行っています。結果を見ると、道路や建物、歩道や木、車やバイクなどを、ある程度正確に判別していることが分かります。このような認識技術が、ディープラーニングによって今後さらに高精度化・高速化すれば、自動運転の実用化もそう遠くないのかもしれません。</p>

<h3 id='h8-5-4'><span class='secno'>8.5.4　</span>Deep Q-Network（強化学習）</h3>
<p><!-- IDX:Deep Q&#45;Network -->人が試行錯誤を経て学ぶように——たとえば、自転車の乗り方など——、コンピュータにも試行錯誤の過程から自立的に学習させようという分野があります。これは、“教師”が寄り添って教える「教師あり学習」とは異なる分野であり、<!-- IDX:強化学習 --><b>強化学習</b>（reinforcement learning）と呼ばれます。</p>
<p>強化学習では、エージェントと呼ばれるものが、環境の状況に応じて行動を選択し、その行動によって環境が変化するというのが基本的な枠組みです。環境の変化によって、エージェントは何らかの報酬を得ます。強化学習での目的は、より良い報酬が得られるようにエージェントの行動指針を決めるという点にあります（<a href='./ch08.xhtml#fig08_26'>図8-26</a>）。</p>
<div class='image' id='fig08_26'>
<img alt='強化学習の基本的な枠組み：エージェントは、より良い報酬を目指して自立的に学習する' src='images/ch08/fig08_26.png'/>
<p class='caption'>
図8-26　強化学習の基本的な枠組み：エージェントは、より良い報酬を目指して自立的に学習する
</p>
</div>
<p><a href='./ch08.xhtml#fig08_26'>図8-26</a>で示す模式図が強化学習の基本的な枠組みですが、ここでの注意点は、報酬とは決められたものではなく、「見込みの報酬」であるという点です。たとえば、テレビゲームの『スーパーマリオブラザーズ』を考えてみると、マリオを右に動かすことによってどれくらいの報酬を得るかということは必ずしも明確ではありません。その場合、ゲームのスコア（コインを取った、敵を倒したなど）やゲームオーバーなどの明確な指標から逆算して、「見込みの報酬」を決める必要があります。これがもし教師あり学習であれば、それぞれの行動に対して“教師”から正しい評価を受けることができます。</p>
<p>ディープラーニングを使った強化学習の手法として、Deep Q-Network（通称、<!-- IDX:DQN --><b>DQN</b>）<u>［44］</u>という手法があります。これは、Q学習と呼ばれる強化学習のアルゴリズムをベースにします。Q学習の詳細は省略しますが、Q学習では最適な行動を決定するために、 最適行動価値関数と呼ばれる関数を決定します。その関数を近似するためにディープラーニング（CNN）を用いるというのがDQNです。</p>
<p>DQNの研究では、テレビゲームを自動的に学習させ、人を超えるレベルの操作を実現した例が報告されています。<a href='./ch08.xhtml#fig08_27'>図8-27</a>に示すように、DQNで使われるCNNは、ゲーム画像のフレーム（4つの連続したフレーム）を入力として、最終的にはゲームのコントローラーの動き（ジョイスティックの移動量やボタン操作の有無）に対して、その動作の“価値”をそれぞれ出力します。</p>
<div class='image' id='fig08_27'>
<img alt='Deep Q-Networkによってテレビゲームの操作を学習する。入力はテレビゲームの画像であり、試行錯誤を経て、プロ顔負けのゲームコントローラー（ジョイスティック）の手さばきを学習する（文献&lt;u&gt;［44］&lt;/u&gt;より引用）' src='images/ch08/fig08_27.png'/>
<p class='caption'>
図8-27　Deep Q-Networkによってテレビゲームの操作を学習する。入力はテレビゲームの画像であり、試行錯誤を経て、プロ顔負けのゲームコントローラー（ジョイスティック）の手さばきを学習する（文献<u>［44］</u>より引用）
</p>
</div>
<p>これまでテレビゲームなどを学習する場合、ゲームの状態（キャラクターの場所など）はあらかじめ抜き出して与えることが一般的でした。しかし、DQNでは、<a href='./ch08.xhtml#fig08_27'>図8-27</a>で示すように、入力データはテレビゲームの画像だけです。これはDQNの特筆すべき点であり、DQNの応用性を格段に高めていると言えます。なぜなら、ゲームごとに設定を変える必要がなく、DQNには単にゲームの画像を与えればよいからです。実際DQNは、『パックマン』や『Atari』など多くのゲームを同じ構成で学習することができ、さらに多くのゲームで人を上回る成績を叩き出したのです。</p>
<div class='note'><table class='note'><tr><td class='center top' rowspan='2'><img alt='[注記]' class='noteicon' src='images/note.png'/></td></tr><tr><td>
<p>人工知能である<!-- IDX:AlphaGo -->AlphaGo<u>［45］</u>が囲碁のチャンピオンを破ったというニュースは大きな注目を集めました。このAlphaGoという技術の内部でも、ディープラーニングと強化学習が用いられています。AlphaGoでは、3,000万個のプロの棋譜を与えて学習させ、さらに、AlphaGo自身が自分自身と対戦することを何度も繰り返しながら、学習を積み重ねたそうです。なお、AlphaGoとDQNは両方とも、GoogleのDeep Mind社によって行われた研究です。今後も、Deep Mind社の活躍には目が離せません。</p>
</td></tr></table></div>

<h2 id='h8-6'><span class='secno'>8.6　</span>まとめ</h2>
<p>本章では、（やや）ディープなCNNを実装し、手書き数字認識において99%を超える高精度な認識結果を得ました。また、ネットワークをディープにすることのモチベーションを語り、最近のディープラーニングがディープな方向へと向かっていることも説明しました。そして、ディープラーニングのトレンドや実用例、また、高速化に向けた研究や未来を感じさせる研究例を紹介しました。</p>
<p>ディープラーニングの分野では、まだまだ分かっていないことが多く、新しい研究が次から次に発表されています。世界中の研究者や技術者たちは、これからも活発に研究を続け、そして、今では想像もつかないような技術が現実化されるでしょう。</p>
<p>最後までお読みいただき、ありがとうございました。読者の皆さんが本書を通じてディープラーニングについての理解を深め、ディープラーニングのおもしろさに気づいてくれたのであれば、著者としてこれ以上の幸せはありません。</p>
<div class='column'>

<h5 id='column-1'>本章で学んだこと</h5>
<ul>
<li>多くの問題では、ネットワークを深くすることで、性能の向上が期待できる。</li>
<li>ILSVRCと呼ばれる画像認識のコンペティションの最近の動向は、ディープラーニングによる手法が上位を独占し、使われるネットワークもディープ化している。</li>
<li>有名なネットワークには、VGG、GoogLeNet、ResNetがある。</li>
<li>GPUや分散学習、ビット精度の削減などによってディープラーニングの高速化を実現できる。</li>
<li>ディープラーニング（ニューラルネットワーク）は、物体認識だけではなく、物体検出やセグメンテーションに利用できる。</li>
<li>ディープラーニングを用いたアプリケーションとして、画像のキャプション生成、画像の生成、強化学習などがある。最近では、自動運転へのディープラーニングの利用も期待されている。</li>
</ul>
</div>
</body>
</html>
