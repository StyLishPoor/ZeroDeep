<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:ops="http://www.idpf.org/2007/ops" xml:lang="ja">
<head>
  <meta charset="UTF-8" />
  <link rel="stylesheet" type="text/css" href="oreilly.css" />
  <meta name="generator" content="Re:VIEW" />
  <title>目次</title>
</head>
<body>
  <nav xmlns:epub="http://www.idpf.org/2007/ops" epub:type="toc" id="toc">
  <h1 class="toc-title">目次</h1>
<ol class="toc-h1">
<li><a href="titlepage.xhtml">　大扉</a></li>
<li><a href="copy.xhtml">　クレジット</a></li>
<li><a href="ch00c.xhtml">　まえがき</a></li>
<li><a href="ch01.xhtml">　1章　Python入門</a></li>
<li><a href="ch01.xhtml#h1-1">　　1.1　Pythonとは</a></li>
<li><a href="ch01.xhtml#h1-2">　　1.2　Pythonのインストール</a></li>
<li><a href="ch01.xhtml#h1-2-1">　　　1.2.1　Pythonのバージョン</a></li>
<li><a href="ch01.xhtml#h1-2-2">　　　1.2.2　使用する外部ライブラリ</a></li>
<li><a href="ch01.xhtml#h1-2-3">　　　1.2.3　Anacondaディストリビューション</a></li>
<li><a href="ch01.xhtml#h1-3">　　1.3　Pythonインタプリタ</a></li>
<li><a href="ch01.xhtml#h1-3-1">　　　1.3.1　算術計算</a></li>
<li><a href="ch01.xhtml#h1-3-2">　　　1.3.2　データ型</a></li>
<li><a href="ch01.xhtml#h1-3-3">　　　1.3.3　変数</a></li>
<li><a href="ch01.xhtml#h1-3-4">　　　1.3.4　リスト</a></li>
<li><a href="ch01.xhtml#h1-3-5">　　　1.3.5　ディクショナリ</a></li>
<li><a href="ch01.xhtml#h1-3-6">　　　1.3.6　ブーリアン</a></li>
<li><a href="ch01.xhtml#h1-3-7">　　　1.3.7　if文</a></li>
<li><a href="ch01.xhtml#h1-3-8">　　　1.3.8　for文</a></li>
<li><a href="ch01.xhtml#h1-3-9">　　　1.3.9　関数</a></li>
<li><a href="ch01.xhtml#h1-4">　　1.4　Pythonスクリプトファイル</a></li>
<li><a href="ch01.xhtml#h1-4-1">　　　1.4.1　ファイルに保存</a></li>
<li><a href="ch01.xhtml#h1-4-2">　　　1.4.2　クラス</a></li>
<li><a href="ch01.xhtml#h1-5">　　1.5　NumPy</a></li>
<li><a href="ch01.xhtml#h1-5-1">　　　1.5.1　NumPyのインポート</a></li>
<li><a href="ch01.xhtml#h1-5-2">　　　1.5.2　NumPy配列の生成</a></li>
<li><a href="ch01.xhtml#h1-5-3">　　　1.5.3　NumPyの算術計算</a></li>
<li><a href="ch01.xhtml#h1-5-4">　　　1.5.4　NumPyのN次元配列</a></li>
<li><a href="ch01.xhtml#h1-5-5">　　　1.5.5　ブロードキャスト</a></li>
<li><a href="ch01.xhtml#h1-5-6">　　　1.5.6　要素へのアクセス</a></li>
<li><a href="ch01.xhtml#h1-6">　　1.6　Matplotlib</a></li>
<li><a href="ch01.xhtml#h1-6-1">　　　1.6.1　単純なグラフの描画</a></li>
<li><a href="ch01.xhtml#h1-6-2">　　　1.6.2　pyplotの機能</a></li>
<li><a href="ch01.xhtml#h1-6-3">　　　1.6.3　画像の表示</a></li>
<li><a href="ch01.xhtml#h1-7">　　1.7　まとめ</a></li>
<li><a href="ch02.xhtml">　2章　パーセプトロン</a></li>
<li><a href="ch02.xhtml#h2-1">　　2.1　パーセプトロンとは</a></li>
<li><a href="ch02.xhtml#h2-2">　　2.2　単純な論理回路</a></li>
<li><a href="ch02.xhtml#h2-2-1">　　　2.2.1　ANDゲート</a></li>
<li><a href="ch02.xhtml#h2-2-2">　　　2.2.2　NANDゲートとORゲート</a></li>
<li><a href="ch02.xhtml#h2-3">　　2.3　パーセプトロンの実装</a></li>
<li><a href="ch02.xhtml#h2-3-1">　　　2.3.1　簡単な実装</a></li>
<li><a href="ch02.xhtml#h2-3-2">　　　2.3.2　重みとバイアスの導入</a></li>
<li><a href="ch02.xhtml#h2-3-3">　　　2.3.3　重みとバイアスによる実装</a></li>
<li><a href="ch02.xhtml#h2-4">　　2.4　パーセプトロンの限界</a></li>
<li><a href="ch02.xhtml#h2-4-1">　　　2.4.1　XORゲート</a></li>
<li><a href="ch02.xhtml#h2-4-2">　　　2.4.2　線形と非線形</a></li>
<li><a href="ch02.xhtml#h2-5">　　2.5　多層パーセプトロン</a></li>
<li><a href="ch02.xhtml#h2-5-1">　　　2.5.1　既存ゲートの組み合わせ</a></li>
<li><a href="ch02.xhtml#h2-5-2">　　　2.5.2　XORゲートの実装</a></li>
<li><a href="ch02.xhtml#h2-6">　　2.6　NANDからコンピュータへ</a></li>
<li><a href="ch02.xhtml#h2-7">　　2.7　まとめ</a></li>
<li><a href="ch03.xhtml">　3章　ニューラルネットワーク</a></li>
<li><a href="ch03.xhtml#h3-1">　　3.1　パーセプトロンからニューラルネットワークへ</a></li>
<li><a href="ch03.xhtml#h3-1-1">　　　3.1.1　ニューラルネットワークの例</a></li>
<li><a href="ch03.xhtml#h3-1-2">　　　3.1.2　パーセプトロンの復習</a></li>
<li><a href="ch03.xhtml#h3-1-3">　　　3.1.3　活性化関数の登場</a></li>
<li><a href="ch03.xhtml#h3-2">　　3.2　活性化関数</a></li>
<li><a href="ch03.xhtml#h3-2-1">　　　3.2.1　シグモイド関数</a></li>
<li><a href="ch03.xhtml#h3-2-2">　　　3.2.2　ステップ関数の実装</a></li>
<li><a href="ch03.xhtml#h3-2-3">　　　3.2.3　ステップ関数のグラフ</a></li>
<li><a href="ch03.xhtml#h3-2-4">　　　3.2.4　シグモイド関数の実装</a></li>
<li><a href="ch03.xhtml#h3-2-5">　　　3.2.5　シグモイド関数とステップ関数の比較</a></li>
<li><a href="ch03.xhtml#h3-2-6">　　　3.2.6　非線形関数</a></li>
<li><a href="ch03.xhtml#h3-2-7">　　　3.2.7　ReLU関数</a></li>
<li><a href="ch03.xhtml#h3-3">　　3.3　多次元配列の計算</a></li>
<li><a href="ch03.xhtml#h3-3-1">　　　3.3.1　多次元配列</a></li>
<li><a href="ch03.xhtml#h3-3-2">　　　3.3.2　行列の内積</a></li>
<li><a href="ch03.xhtml#h3-3-3">　　　3.3.3　ニューラルネットワークの内積</a></li>
<li><a href="ch03.xhtml#h3-4">　　3.4　3層ニューラルネットワークの実装</a></li>
<li><a href="ch03.xhtml#h3-4-1">　　　3.4.1　記号の確認</a></li>
<li><a href="ch03.xhtml#h3-4-2">　　　3.4.2　各層における信号伝達の実装</a></li>
<li><a href="ch03.xhtml#h3-4-3">　　　3.4.3　実装のまとめ</a></li>
<li><a href="ch03.xhtml#h3-5">　　3.5　出力層の設計</a></li>
<li><a href="ch03.xhtml#h3-5-1">　　　3.5.1　恒等関数とソフトマックス関数</a></li>
<li><a href="ch03.xhtml#h3-5-2">　　　3.5.2　ソフトマックス関数の実装上の注意</a></li>
<li><a href="ch03.xhtml#h3-5-3">　　　3.5.3　ソフトマックス関数の特徴</a></li>
<li><a href="ch03.xhtml#h3-5-4">　　　3.5.4　出力層のニューロンの数</a></li>
<li><a href="ch03.xhtml#h3-6">　　3.6　手書き数字認識</a></li>
<li><a href="ch03.xhtml#h3-6-1">　　　3.6.1　MNISTデータセット</a></li>
<li><a href="ch03.xhtml#h3-6-2">　　　3.6.2　ニューラルネットワークの推論処理</a></li>
<li><a href="ch03.xhtml#h3-6-3">　　　3.6.3　バッチ処理</a></li>
<li><a href="ch03.xhtml#h3-7">　　3.7　まとめ</a></li>
<li><a href="ch04.xhtml">　4章　ニューラルネットワークの学習</a></li>
<li><a href="ch04.xhtml#h4-1">　　4.1　データから学習する</a></li>
<li><a href="ch04.xhtml#h4-1-1">　　　4.1.1　データ駆動</a></li>
<li><a href="ch04.xhtml#h4-1-2">　　　4.1.2　訓練データとテストデータ</a></li>
<li><a href="ch04.xhtml#h4-2">　　4.2　損失関数</a></li>
<li><a href="ch04.xhtml#h4-2-1">　　　4.2.1　2乗和誤差</a></li>
<li><a href="ch04.xhtml#h4-2-2">　　　4.2.2　交差エントロピー誤差</a></li>
<li><a href="ch04.xhtml#h4-2-3">　　　4.2.3　ミニバッチ学習</a></li>
<li><a href="ch04.xhtml#h4-2-4">　　　4.2.4　［バッチ対応版］交差エントロピー誤差の実装</a></li>
<li><a href="ch04.xhtml#h4-2-5">　　　4.2.5　なぜ損失関数を設定するのか？</a></li>
<li><a href="ch04.xhtml#h4-3">　　4.3　数値微分</a></li>
<li><a href="ch04.xhtml#h4-3-1">　　　4.3.1　微分</a></li>
<li><a href="ch04.xhtml#h4-3-2">　　　4.3.2　数値微分の例</a></li>
<li><a href="ch04.xhtml#h4-3-3">　　　4.3.3　偏微分</a></li>
<li><a href="ch04.xhtml#h4-4">　　4.4　勾配</a></li>
<li><a href="ch04.xhtml#h4-4-1">　　　4.4.1　勾配法</a></li>
<li><a href="ch04.xhtml#h4-4-2">　　　4.4.2　ニューラルネットワークに対する勾配</a></li>
<li><a href="ch04.xhtml#h4-5">　　4.5　学習アルゴリズムの実装</a></li>
<li><a href="ch04.xhtml#h4-5-1">　　　4.5.1　2層ニューラルネットワークのクラス</a></li>
<li><a href="ch04.xhtml#h4-5-2">　　　4.5.2　ミニバッチ学習の実装</a></li>
<li><a href="ch04.xhtml#h4-5-3">　　　4.5.3　テストデータで評価</a></li>
<li><a href="ch04.xhtml#h4-6">　　4.6　まとめ</a></li>
<li><a href="ch05.xhtml">　5章　誤差逆伝播法</a></li>
<li><a href="ch05.xhtml#h5-1">　　5.1　計算グラフ</a></li>
<li><a href="ch05.xhtml#h5-1-1">　　　5.1.1　計算グラフで解く</a></li>
<li><a href="ch05.xhtml#h5-1-2">　　　5.1.2　局所的な計算</a></li>
<li><a href="ch05.xhtml#h5-1-3">　　　5.1.3　なぜ計算グラフで解くのか？</a></li>
<li><a href="ch05.xhtml#h5-2">　　5.2　連鎖率</a></li>
<li><a href="ch05.xhtml#h5-2-1">　　　5.2.1　計算グラフの逆伝播</a></li>
<li><a href="ch05.xhtml#h5-2-2">　　　5.2.2　連鎖率とは</a></li>
<li><a href="ch05.xhtml#h5-2-3">　　　5.2.3　連鎖率と計算グラフ</a></li>
<li><a href="ch05.xhtml#h5-3">　　5.3　逆伝播</a></li>
<li><a href="ch05.xhtml#h5-3-1">　　　5.3.1　加算ノードの逆伝播</a></li>
<li><a href="ch05.xhtml#h5-3-2">　　　5.3.2　乗算ノードの逆伝播</a></li>
<li><a href="ch05.xhtml#h5-3-3">　　　5.3.3　リンゴの例</a></li>
<li><a href="ch05.xhtml#h5-4">　　5.4　単純なレイヤの実装</a></li>
<li><a href="ch05.xhtml#h5-4-1">　　　5.4.1　乗算レイヤの実装</a></li>
<li><a href="ch05.xhtml#h5-4-2">　　　5.4.2　加算レイヤの実装</a></li>
<li><a href="ch05.xhtml#h5-5">　　5.5　活性化関数レイヤの実装</a></li>
<li><a href="ch05.xhtml#h5-5-1">　　　5.5.1　ReLUレイヤ</a></li>
<li><a href="ch05.xhtml#h5-5-2">　　　5.5.2　Sigmoidレイヤ</a></li>
<li><a href="ch05.xhtml#h5-6">　　5.6　Affine／Softmaxレイヤの実装</a></li>
<li><a href="ch05.xhtml#h5-6-1">　　　5.6.1　Affineレイヤ</a></li>
<li><a href="ch05.xhtml#h5-6-2">　　　5.6.2　バッチ版Affineレイヤ</a></li>
<li><a href="ch05.xhtml#h5-6-3">　　　5.6.3　Softmax-with-Lossレイヤ</a></li>
<li><a href="ch05.xhtml#h5-7">　　5.7　誤差逆伝播法の実装</a></li>
<li><a href="ch05.xhtml#h5-7-1">　　　5.7.1　ニューラルネットワークの学習の全体図</a></li>
<li><a href="ch05.xhtml#h5-7-2">　　　5.7.2　誤差逆伝播法に対応したニューラルネットワークの実装</a></li>
<li><a href="ch05.xhtml#h5-7-3">　　　5.7.3　誤差逆伝播法の勾配確認</a></li>
<li><a href="ch05.xhtml#h5-7-4">　　　5.7.4　誤差逆伝播法を使った学習</a></li>
<li><a href="ch05.xhtml#h5-8">　　5.8　まとめ</a></li>
<li><a href="ch06.xhtml">　6章　学習に関するテクニック</a></li>
<li><a href="ch06.xhtml#h6-1">　　6.1　パラメータの更新</a></li>
<li><a href="ch06.xhtml#h6-1-1">　　　6.1.1　冒険家の話</a></li>
<li><a href="ch06.xhtml#h6-1-2">　　　6.1.2　SGD</a></li>
<li><a href="ch06.xhtml#h6-1-3">　　　6.1.3　SGDの欠点</a></li>
<li><a href="ch06.xhtml#h6-1-4">　　　6.1.4　Momentum</a></li>
<li><a href="ch06.xhtml#h6-1-5">　　　6.1.5　AdaGrad</a></li>
<li><a href="ch06.xhtml#h6-1-6">　　　6.1.6　Adam</a></li>
<li><a href="ch06.xhtml#h6-1-7">　　　6.1.7　どの更新手法を用いるか？</a></li>
<li><a href="ch06.xhtml#h6-1-8">　　　6.1.8　MNISTデータセットによる更新手法の比較</a></li>
<li><a href="ch06.xhtml#h6-2">　　6.2　重みの初期値</a></li>
<li><a href="ch06.xhtml#h6-2-1">　　　6.2.1　重みの初期値を0にする？</a></li>
<li><a href="ch06.xhtml#h6-2-2">　　　6.2.2　隠れ層のアクティベーション分布</a></li>
<li><a href="ch06.xhtml#h6-2-3">　　　6.2.3　ReLUの場合の重みの初期値</a></li>
<li><a href="ch06.xhtml#h6-2-4">　　　6.2.4　MNISTデータセットによる重み初期値の比較</a></li>
<li><a href="ch06.xhtml#h6-3">　　6.3　Batch Normalization</a></li>
<li><a href="ch06.xhtml#h6-3-1">　　　6.3.1　Batch Normalizationのアルゴリズム</a></li>
<li><a href="ch06.xhtml#h6-3-2">　　　6.3.2　Batch Normalizationの評価</a></li>
<li><a href="ch06.xhtml#h6-4">　　6.4　正則化</a></li>
<li><a href="ch06.xhtml#h6-4-1">　　　6.4.1　過学習</a></li>
<li><a href="ch06.xhtml#h6-4-2">　　　6.4.2　Weight decay</a></li>
<li><a href="ch06.xhtml#h6-4-3">　　　6.4.3　Dropout</a></li>
<li><a href="ch06.xhtml#h6-5">　　6.5　ハイパーパラメータの検証</a></li>
<li><a href="ch06.xhtml#h6-5-1">　　　6.5.1　検証データ</a></li>
<li><a href="ch06.xhtml#h6-5-2">　　　6.5.2　ハイパーパラメータの最適化</a></li>
<li><a href="ch06.xhtml#h6-5-3">　　　6.5.3　ハイパーパラメータ最適化の実装</a></li>
<li><a href="ch06.xhtml#h6-6">　　6.6　まとめ</a></li>
<li><a href="ch07.xhtml">　7章　畳み込みニューラルネットワーク</a></li>
<li><a href="ch07.xhtml#h7-1">　　7.1　全体の構造</a></li>
<li><a href="ch07.xhtml#h7-2">　　7.2　畳み込み層</a></li>
<li><a href="ch07.xhtml#h7-2-1">　　　7.2.1　全結合層の問題点</a></li>
<li><a href="ch07.xhtml#h7-2-2">　　　7.2.2　畳み込み演算</a></li>
<li><a href="ch07.xhtml#h7-2-3">　　　7.2.3　パディング</a></li>
<li><a href="ch07.xhtml#h7-2-4">　　　7.2.4　ストライド</a></li>
<li><a href="ch07.xhtml#h7-2-5">　　　7.2.5　3次元データの畳み込み演算</a></li>
<li><a href="ch07.xhtml#h7-2-6">　　　7.2.6　ブロックで考える</a></li>
<li><a href="ch07.xhtml#h7-2-7">　　　7.2.7　バッチ処理</a></li>
<li><a href="ch07.xhtml#h7-3">　　7.3　プーリング層</a></li>
<li><a href="ch07.xhtml#h7-3-1">　　　7.3.1　プーリング層の特徴</a></li>
<li><a href="ch07.xhtml#h7-4">　　7.4　Convolution／Poolingレイヤの実装</a></li>
<li><a href="ch07.xhtml#h7-4-1">　　　7.4.1　4次元配列</a></li>
<li><a href="ch07.xhtml#h7-4-2">　　　7.4.2　im2colによる展開</a></li>
<li><a href="ch07.xhtml#h7-4-3">　　　7.4.3　Convolutionレイヤの実装</a></li>
<li><a href="ch07.xhtml#h7-4-4">　　　7.4.4　Poolingレイヤの実装</a></li>
<li><a href="ch07.xhtml#h7-5">　　7.5　CNNの実装</a></li>
<li><a href="ch07.xhtml#h7-6">　　7.6　CNNの可視化</a></li>
<li><a href="ch07.xhtml#h7-6-1">　　　7.6.1　1層目の重みの可視化</a></li>
<li><a href="ch07.xhtml#h7-6-2">　　　7.6.2　階層構造による情報抽出</a></li>
<li><a href="ch07.xhtml#h7-7">　　7.7　代表的なCNN</a></li>
<li><a href="ch07.xhtml#h7-7-1">　　　7.7.1　LeNet</a></li>
<li><a href="ch07.xhtml#h7-7-2">　　　7.7.2　AlexNet</a></li>
<li><a href="ch07.xhtml#h7-8">　　7.8　まとめ</a></li>
<li><a href="ch08.xhtml">　8章　ディープラーニング</a></li>
<li><a href="ch08.xhtml#h8-1">　　8.1　ネットワークをより深く</a></li>
<li><a href="ch08.xhtml#h8-1-1">　　　8.1.1　よりディープなネットワークへ</a></li>
<li><a href="ch08.xhtml#h8-1-2">　　　8.1.2　さらに認識精度を高めるには</a></li>
<li><a href="ch08.xhtml#h8-1-3">　　　8.1.3　層を深くすることのモチベーション</a></li>
<li><a href="ch08.xhtml#h8-2">　　8.2　ディープラーニングの小歴史</a></li>
<li><a href="ch08.xhtml#h8-2-1">　　　8.2.1　ImageNet</a></li>
<li><a href="ch08.xhtml#h8-2-2">　　　8.2.2　VGG</a></li>
<li><a href="ch08.xhtml#h8-2-3">　　　8.2.3　GoogLeNet</a></li>
<li><a href="ch08.xhtml#h8-2-4">　　　8.2.4　ResNet</a></li>
<li><a href="ch08.xhtml#h8-3">　　8.3　ディープラーニングの高速化</a></li>
<li><a href="ch08.xhtml#h8-3-1">　　　8.3.1　取り組むべき問題</a></li>
<li><a href="ch08.xhtml#h8-3-2">　　　8.3.2　GPUによる高速化</a></li>
<li><a href="ch08.xhtml#h8-3-3">　　　8.3.3　分散学習</a></li>
<li><a href="ch08.xhtml#h8-3-4">　　　8.3.4　演算精度のビット削減</a></li>
<li><a href="ch08.xhtml#h8-4">　　8.4　ディープラーニングの実用例</a></li>
<li><a href="ch08.xhtml#h8-4-1">　　　8.4.1　物体検出</a></li>
<li><a href="ch08.xhtml#h8-4-2">　　　8.4.2　セグメンテーション</a></li>
<li><a href="ch08.xhtml#h8-4-3">　　　8.4.3　画像キャプション生成</a></li>
<li><a href="ch08.xhtml#h8-5">　　8.5　ディープラーニングの未来</a></li>
<li><a href="ch08.xhtml#h8-5-1">　　　8.5.1　画像スタイル変換</a></li>
<li><a href="ch08.xhtml#h8-5-2">　　　8.5.2　画像生成</a></li>
<li><a href="ch08.xhtml#h8-5-3">　　　8.5.3　自動運転</a></li>
<li><a href="ch08.xhtml#h8-5-4">　　　8.5.4　Deep Q-Network（強化学習）</a></li>
<li><a href="ch08.xhtml#h8-6">　　8.6　まとめ</a></li>
<li><a href="ch11a.xhtml">　付録A　Softmax-with-Lossレイヤの計算グラフ</a></li>
<li><a href="ch11a.xhtml#hA-1">　　A.1　順伝播</a></li>
<li><a href="ch11a.xhtml#hA-2">　　A.2　逆伝播</a></li>
<li><a href="ch11a.xhtml#hA-3">　　A.3　まとめ</a></li>
<li><a href="ch18ref.xhtml">　参考文献</a></li>
<li><a href="ch18ref.xhtml#h-0-1">　　　Python / NumPy</a></li>
<li><a href="ch18ref.xhtml#h-0-2">　　　計算グラフ（誤差逆伝播法）</a></li>
<li><a href="ch18ref.xhtml#h-0-3">　　　Deep Learningのオンライン授業（資料）</a></li>
<li><a href="ch18ref.xhtml#h-0-4">　　　パラメータの更新方法</a></li>
<li><a href="ch18ref.xhtml#h-0-5">　　　重みパラメータの初期値</a></li>
<li><a href="ch18ref.xhtml#h-0-6">　　　Batch Normalization / Dropout</a></li>
<li><a href="ch18ref.xhtml#h-0-7">　　　ハイパーパラメータの最適化</a></li>
<li><a href="ch18ref.xhtml#h-0-8">　　　CNNの可視化</a></li>
<li><a href="ch18ref.xhtml#h-0-9">　　　代表的なネットワーク</a></li>
<li><a href="ch18ref.xhtml#h-0-10">　　　データセット</a></li>
<li><a href="ch18ref.xhtml#h-0-11">　　　計算の高速化</a></li>
<li><a href="ch18ref.xhtml#h-0-12">　　　MNISTデータセットの精度ランキングおよび最高精度の手法</a></li>
<li><a href="ch18ref.xhtml#h-0-13">　　　ディープラーニングのアプリケーション</a></li>
<li><a href="author.xhtml">　著者紹介</a></li>
<li><a href="colophon.xhtml">　奥付</a></li>
</ol>
  </nav>
</body>
</html>
